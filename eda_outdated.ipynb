{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-14T07:24:39.144058Z",
     "iopub.status.busy": "2026-01-14T07:24:39.143539Z",
     "iopub.status.idle": "2026-01-14T07:24:40.187029Z",
     "shell.execute_reply": "2026-01-14T07:24:40.185522Z",
     "shell.execute_reply.started": "2026-01-14T07:24:39.143837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\"\"\"for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\"\"\"\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "## Simulating stock market movement over quarters with varied market character; can our agents successfully predict market price for all of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:24:40.189584Z",
     "iopub.status.busy": "2026-01-14T07:24:40.188633Z",
     "iopub.status.idle": "2026-01-14T07:24:58.391369Z",
     "shell.execute_reply": "2026-01-14T07:24:58.390075Z",
     "shell.execute_reply.started": "2026-01-14T07:24:40.189541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles before date cleaning: 1400469\n",
      "Total articles after date cleaning: 1397891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_45560\\2345186197.py:16: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  all_news_df['q_index'] = all_news_df['date'].dt.to_period('Q').astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Let's create dataframes for each quarter\n",
    "# 1. Get all news\n",
    "all_news_df = pd.read_csv(\"dataset/analyst_ratings_processed.csv\")\n",
    "\n",
    "print(f\"Total articles before date cleaning: {len(all_news_df)}\")\n",
    "\n",
    "# 2. Convert to datetime, forcing errors to NaT (Not a Time)\n",
    "all_news_df['date'] = pd.to_datetime(all_news_df['date'], errors='coerce', utc=True)\n",
    "\n",
    "# 3. Drop the rows where the date was invalid\n",
    "all_news_df = all_news_df.dropna(subset=['date'])\n",
    "\n",
    "print(f\"Total articles after date cleaning: {len(all_news_df)}\")\n",
    "\n",
    "# 4. Create a 'quarter' helper column (format: '2009Q2')\n",
    "all_news_df['q_index'] = all_news_df['date'].dt.to_period('Q').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:24:58.393303Z",
     "iopub.status.busy": "2026-01-14T07:24:58.392940Z",
     "iopub.status.idle": "2026-01-14T07:24:58.492528Z",
     "shell.execute_reply": "2026-01-14T07:24:58.490709Z",
     "shell.execute_reply.started": "2026-01-14T07:24:58.393272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   q_index  count\n",
      "0   2020Q1  59849\n",
      "1   2020Q2  45826\n",
      "2   2019Q4  42475\n",
      "3   2019Q3  41954\n",
      "4   2018Q3  41612\n",
      "5   2018Q4  41611\n",
      "6   2014Q4  36368\n",
      "7   2016Q1  35906\n",
      "8   2016Q3  35536\n",
      "9   2011Q3  35318\n",
      "10  2015Q1  35066\n",
      "11  2015Q2  34961\n",
      "12  2016Q4  34943\n",
      "13  2016Q2  34929\n",
      "14  2011Q2  34284\n",
      "15  2010Q4  34223\n",
      "16  2019Q2  33800\n",
      "17  2014Q3  32911\n",
      "18  2012Q1  32570\n",
      "19  2015Q4  32098\n",
      "20  2013Q1  32064\n",
      "21  2017Q1  32047\n",
      "22  2018Q2  32004\n",
      "23  2019Q1  31851\n",
      "24  2014Q2  31590\n",
      "25  2011Q4  31438\n",
      "26  2017Q4  31314\n",
      "27  2011Q1  31302\n",
      "28  2018Q1  31186\n",
      "29  2013Q2  31117\n",
      "30  2012Q2  30926\n",
      "31  2015Q3  30752\n",
      "32  2013Q3  29751\n",
      "33  2017Q2  29664\n",
      "34  2012Q4  29649\n",
      "35  2014Q1  29082\n",
      "36  2012Q3  29067\n",
      "37  2013Q4  28340\n",
      "38  2017Q3  27274\n",
      "39  2010Q3  20648\n",
      "40  2010Q2  13675\n",
      "41  2010Q1  12590\n",
      "42  2009Q3   7590\n",
      "43  2009Q4   6677\n",
      "44  2009Q2     52\n",
      "45  2009Q1      1\n"
     ]
    }
   ],
   "source": [
    "# Let's look at how much data we have for each quarter\n",
    "articles_by_q = all_news_df['q_index'].value_counts(dropna=False).reset_index()\n",
    "print(articles_by_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the above results, have decided on the following quarters:\n",
    "* 2012Q1\n",
    "> 32,570 articles, can we beat a basic bull market?\n",
    "* 2014Q4\n",
    "> 36,368 articles, can we handle sector-specific shocks (Oil)?\n",
    "* 2016Q1\n",
    "> 35,906 articles, can we survive a 10% dip and stay in the game?\n",
    "* 2018Q4\n",
    "> 41,611 articles, can we manage a sharp, high-volume year-end crash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:24:58.498046Z",
     "iopub.status.busy": "2026-01-14T07:24:58.497695Z",
     "iopub.status.idle": "2026-01-14T07:24:59.132315Z",
     "shell.execute_reply": "2026-01-14T07:24:59.130695Z",
     "shell.execute_reply.started": "2026-01-14T07:24:58.498015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32570 36368 35906 41611\n"
     ]
    }
   ],
   "source": [
    "# What stocks do we need to keep track of for each quarter? Let's split up the original df to find out.\n",
    "q1_2012 = all_news_df[all_news_df['q_index'] == '2012Q1'].copy()\n",
    "q4_2014 = all_news_df[all_news_df['q_index'] == '2014Q4'].copy()\n",
    "q1_2016 = all_news_df[all_news_df['q_index'] == '2016Q1'].copy()\n",
    "q4_2018 = all_news_df[all_news_df['q_index'] == '2018Q4'].copy()\n",
    "\n",
    "# Verify counts\n",
    "print(len(q1_2012), len(q4_2014), len(q1_2016), len(q4_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:24:59.134798Z",
     "iopub.status.busy": "2026-01-14T07:24:59.134249Z",
     "iopub.status.idle": "2026-01-14T07:24:59.173414Z",
     "shell.execute_reply": "2026-01-14T07:24:59.172007Z",
     "shell.execute_reply.started": "2026-01-14T07:24:59.134757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     stock  count\n",
      "0      QQQ    538\n",
      "1      NBG    329\n",
      "2     ZNGA    151\n",
      "3     VVUS    145\n",
      "4     GMCR    139\n",
      "...    ...    ...\n",
      "3352  SNAK      1\n",
      "3353  SMTX      1\n",
      "3354   SMN      1\n",
      "3355   CTV      1\n",
      "3356   CTT      1\n",
      "\n",
      "[3357 rows x 2 columns]\n",
      "     stock  count\n",
      "0     GPRO    224\n",
      "1     TKMR    220\n",
      "2     LAKE    188\n",
      "3     EBAY    177\n",
      "4     NLNK    177\n",
      "...    ...    ...\n",
      "4116  ABCW      1\n",
      "4117  EMLP      1\n",
      "4118  XNET      1\n",
      "4119  XNCR      1\n",
      "4120   XME      1\n",
      "\n",
      "[4121 rows x 2 columns]\n",
      "     stock  count\n",
      "0     GPRO    253\n",
      "1      CMG    220\n",
      "2     YHOO    211\n",
      "3      TWX    146\n",
      "4     BABA    135\n",
      "...    ...    ...\n",
      "3995   KHI      1\n",
      "3996   KEM      1\n",
      "3997  EUDG      1\n",
      "3998   ETM      1\n",
      "3999  PFNX      1\n",
      "\n",
      "[4000 rows x 2 columns]\n",
      "      stock  count\n",
      "0       DIA    329\n",
      "1      NFLX    178\n",
      "2     GOOGL    176\n",
      "3       XRT    174\n",
      "4       EWW    163\n",
      "...     ...    ...\n",
      "3732   SLCT      1\n",
      "3733    SRT      1\n",
      "3734    DBV      1\n",
      "3735    DNN      1\n",
      "3736    DKT      1\n",
      "\n",
      "[3737 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count stock tickers for each quarter\n",
    "\n",
    "q1_2012_stocks = q1_2012['stock'].value_counts(dropna=False).reset_index()\n",
    "q4_2014_stocks = q4_2014['stock'].value_counts(dropna=False).reset_index()\n",
    "q1_2016_stocks = q1_2016['stock'].value_counts(dropna=False).reset_index()\n",
    "q4_2018_stocks = q4_2018['stock'].value_counts(dropna=False).reset_index()\n",
    "\n",
    "print(q1_2012_stocks)\n",
    "print(q4_2014_stocks)\n",
    "print(q1_2016_stocks)\n",
    "print(q4_2018_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:12:42.527189Z",
     "iopub.status.busy": "2026-01-14T08:12:42.526793Z",
     "iopub.status.idle": "2026-01-14T08:12:42.847883Z",
     "shell.execute_reply": "2026-01-14T08:12:42.846699Z",
     "shell.execute_reply.started": "2026-01-14T08:12:42.527155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(1796405), np.float64(1.0778429999999999))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cost Analysis: How many tokens will be used to embed all of these news articles?\n",
    "\n",
    "# Function to count words safely\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):\n",
    "        # Split by whitespace and filter out empty strings\n",
    "        return len([word for word in text.strip().split() if word])\n",
    "    return 0  # For NaN or non-string values\n",
    "\n",
    "# Apply function to column\n",
    "q4_2018['word_count'] = q4_2018['title'].apply(count_words) + 1 \n",
    "q1_2016['word_count'] = q1_2016['title'].apply(count_words) + 1 \n",
    "q4_2014['word_count'] = q4_2014['title'].apply(count_words) + 1 \n",
    "q1_2012['word_count'] = q1_2012['title'].apply(count_words) + 1 \n",
    "\n",
    "# Total number of words in the column\n",
    "total_words = q4_2018['word_count'].sum() + q1_2016['word_count'].sum() + q4_2014['word_count'].sum() + q1_2012['word_count'].sum()\n",
    "\n",
    "avg_tk_per_wd = 4\n",
    "cost_per_m_tokens = 0.15\n",
    "projected_cost = total_words * avg_tk_per_wd * (cost_per_m_tokens / 1000000)\n",
    "total_words, projected_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# How long is the longest document? If it's just headlines, can we embed without chunking?\n",
    "max_q4_2018 = q4_2018['word_count'].max()\n",
    "max_q1_2016 = q1_2016['word_count'].max()\n",
    "max_q4_2014 = q4_2014['word_count'].max()\n",
    "max_q1_2012 = q1_2012['word_count'].max()\n",
    "print(max(max_q4_2018, max_q1_2016, max_q4_2014, max_q1_2012)) # 59 words"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 754810,
     "sourceId": 1304644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "reuthing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
